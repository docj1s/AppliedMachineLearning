---
title: "Project"
author: "Rashaad Jones"
date: "January 7, 2018"
output: html_document
--

# Introduction
Using data from exercise devices (e.g., FitBit, Jawbone Up, etc), predict the manner in which a person performed the exercise.  More information regarding the data can be found at http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.  

# Model Development
Models used were a (1) Decision Tree model and a (2) Random Forest model.  These were chosen because they are suitable candidates for this type of classification problem.  All data was used except for columns that had NA values AND superfluous features (i.e., user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and  num_window (columns 1 to 7)).  These features cannot be used to predict outcome.      

# Cross-validation

Cross-validation will be performed by subsampling training data set randomly without replacement into 2 subsamples: subTraining data (70% of the original Training data set) and subTesting data (25%).  The models will be fitted on the subTraining data set, and tested on the subTesting data. Once the most accurate model is choosen, it will be tested on the original testing data set.

# Expected out-of-sample error

The expected out-of-sample error will correspond to the quantity: 1-accuracy in the cross-validation data. The accuracy is the proportion of correct classified observations over the total sample in the subTesting data set. The expected accuracy is the accuracy in the out-of-sample data set. The expected value of the out-of-sample error will correspond to the expected number of missclassified observations/total observations in the test data set.  Specifically, the random forest model is expected to perform better than the decision tree model and will be used to predict against the final test set.  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r}

#install.packages("caret")
#install.packages("randomForest")
#install.packages("rpart")

library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)

set.seed(2245) #for reproduceability

#training <- read.csv("pml-training.csv", na.strings=c("NA", "#DIV/0!", ""))
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na.strings=c("NA", "#DIV/0!", ""))

training <- training[, colSums(is.na(training)) ==0]

# Remove superfluous features
training   <-training[,-c(1:7)]

indexes <- createDataPartition(y=training$classe, p=.7, list=FALSE)

subTraining <- training[indexes, ]
subTesting <- training[-indexes, ]

plot(subTraining$classe, col="blue", main="Freq vs Classe Level (subtraining only)", xlab="Classe Levels", ylab="Frequency")

fit.dt <- rpart(classe ~ ., data = subTraining, method="class")

fit.predictionDT <- predict(fit.dt, subTesting, type="class")

rpart.plot(fit.dt, main="Decision Tree", extra=102, under=TRUE, faclen=0)

confusionMatrix(fit.predictionDT, subTesting$classe)

fit.rf <- randomForest(classe ~., data=subTraining, method="class")

fit.predictionRF <- predict(fit.rf, subTesting, type="class")

confusionMatrix(fit.predictionRF, subTesting$classe)
```

## Using the test data with Random Forest
```{r}
#testing <- read.csv("pml-testing.csv", na.strings=c("NA", "#DIV/0!", ""))
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na.strings=c("NA", "#DIV/0!", ""))

testing <- testing[, colSums(is.na(testing)) ==0]
testing <-testing[,-c(1:7)]
 
fit.predictionFinal <- predict(fit.rf, testing, type="class")

fit.predictionFinal
```

